▼第1章 はじめに
　1 特徴抽出→学習により、識別能力を得る
　2 特徴には、名義尺度・順序尺度・間隔尺度・比例尺度があり、各々の性質を理解して使うこと。定性的な特徴はダミー変数として二値化する
　3 パターン認識では特徴量をベクトルで表し、特徴量次元の空間内の各点として要素を表す


▼第2章 識別規則と学習法の概要
　1.1 識別規則は大別して4種類ある
　　1.事後確率→どのクラスに属するのが最も確率が高いかを確率分布から求める
　　2.距離→どのクラスの代表ベクトルと最も近いかを求める
　　3.関数値→f(x)の結果が、正か負か、または最大値で分類する
　　4.決定木→識別規則に従って分類する
　1.2 教師あり学習では、データセットxに対する関数 y=f(x)　において、y=-1,+1となるように分類する。
　　多クラスになる場合は、yは0と1の行列になるように定義する。
　　学習ではデータセットを繰り返し学習させることで、正解である教師データに近くなるよう関数を調整する。
　　ある程度まで精度が収束してきたら、教師データ以外のテストデータでバリデーション（評価）を行う
　1.3 二値ではなく、関数値を得る方法を、関数近似（線形回帰）という。
　1.4 類似度や統計量で自発的にクラス分類することを、教師なし学習という。全てのデータに教師を付けるのではなく、一部のデータに教師あり、残りは教師なしとして、形質導入学習という手法もある。

　2.1 データは教師データとテストデータに分ける。学習機を用いて、教師データを間違える確率を再代入誤り率、テストデータを間違える確率をホールドアウト誤り率という。再代入誤り率 <= 真の誤り率 <= ホールドアウト誤り率 という関係式が成り立つ。
　　データセットをいくつかに分割し、そのうちの1つをテストデータとして使うことを、全ての塊に対して実行し、平均値をとることをクロスバリデーションという。データの分割の方法で偏りが出るので、繰り返しやった方が良い。
　　データの数と分割グループを同じくしたクロスバリデーションを、1つ抜き法という。
　　再代入誤り率と、真の誤り率との差分をバイアスという。このバイアスを補正するものが、ブートストラップ法である。N個のデータ群に分けて、N回復元抽出したデータを使ってブートスラップサンプルを作る。
　　学習機のモデル選択の際には、分散とバイアスの両方のバランスを考える必要がある。たとえば、関数近似の場合、複雑な関数にした方が学習データへの精度は上がるが、テストデータへの精度は悪くなってしまう。（過学習）これは、複雑な関数がノイズを追随してしまうためである。

▼第3章 ベイズの識別規則
　事後確率＝尤度x事前確率で表される。この事後確率を最大にするようなクラスが、識別クラスになる。
　あるサンプルが、あるクラスに属している確率分布を図示すると、境界値の部分で分割するのが誤り率が最も小さくなることがわかる。
　ただし、誤りが発生したとき、どう間違えるかで損失が大きく変わる場合があるので、単純に面積が最小のところが最も小さいとは言えない。損失を考慮すると、損失の小さい方に境界値がずれる。
　境界値付近にサンプルが来てしまい、誤りが生じる可能性が高い（閾値による）場合は、判断を留保してリジェクトする手法がある。
　上記のベイズ誤り率を識別性能として使うためには、事前確率・尤度・識別境界を知る必要がある。
　それらを使わない評価方法として、受信者動作特性曲線(ROC曲線)というものがある。
　ROC曲線とは、偽陽性率と真陽性率の関係をグラフにしたもの。正しく、ほしい情報を提示する確率と、ほしい情報を誤って提示する確率の関係。
　完全にランダムな判定器だと、y=xの直線になる。
　ROC曲線の下側の面積を、ROC曲線下面積（AUC）とよび、識別機の評価尺度として使われている。0.5<=AUC<=1.0

▼第4章 確率モデルと識別関数
　特徴量間に相関があると、どちらか一方で事足りるので、相関はなくしたほうがよい。
　そのため、線形変換として標準化・無相関化・白色化がある。
　パラメータの相関は、各々の分散と共分散を用いて計算する。
　標準化：パラメータの平均を0、分散を1にならす方法
　無相関化：主成分分析と密接に関係。観測データの分布を回転させることで相関をなくす。要は固有ベクトルを作って正規直行規定を作るようにするらしいけど、このあたりはよくわからん。
　白色化：全パラメータの標準偏差を1にして、中心化したもの。無相関化＋標準化の操作に該当する。データの分布がどの方向に対しても単位超球上に並ぶことが特徴。
　確率モデルには、パラメトリックモデルとノンパラメトリックモデルに大別できる。
　パラメトリックはある統計モデルに当てはまっていると仮定してモデル化する方法で、ノンパラメトリックモデルは学習データからモデルを推定する方法。
　パラメトリックモデルの代表例：（離散的）二項分布、多項分布、ポアソン分布　（連続的）正規分布、指数分布、一様分布
　ノンパラメトリックモデルの代表例：ヒストグラム法、k最近傍法（kNN）、パルツェン密度推定法
　正規分布関数と、正規分布から導かれる識別関数について。
　
　



▼第5章 k最近傍法(kNN法)




▼第6章 線形識別関数




▼第7章 パーセプトロン型学習規則




▼第8章 サポートベクトルマシン



▼第9章 部分空間法


▼第10章 クラスタリング


▼第11章 識別機の組み合わせによる性能強化



#　メモ
ブートストラップ法よくわからん。
ブートストラップサンプルの作り方がよくわかってない？
→ブートストラップサンプルは、N個の集合の中から、N回復元抽出をして作ったサンプル
　→復元抽出とは、選んだものを元の集合に戻して、もう一度サンプルを選ぶ方法。

事前確率、事後確率って？
→事象が発生する前のわかっている確率が事前確率、条件付きで発生する確率が事後確率

4章までで一旦まとめる。